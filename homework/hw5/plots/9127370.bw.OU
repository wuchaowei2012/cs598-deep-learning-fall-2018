----------------------------------------
Begin Torque Prologue on nid25428
at Sat Oct 20 23:51:06 CDT 2018
Job Id:			9127370.bw
Username:		tra479
Group:			TRAIN_bauh
Job name:		hw5_resnet50_2
Requested resources:	nodes=1:ppn=16:xk,walltime=40:00:00,neednodes=1:ppn=16:xk
Queue:			normal
Account:		bauh
End Torque Prologue:  0.167 elapsed
----------------------------------------




==> Peparing files...
==> Loading data...
	number of workers: 16
==> Loading pretrained resnet50 model...
==> Loading checkpoint...
Loading model from disk...
==> Start training on device cuda...
	Hyperparameters: LR = 0.001, EPOCHS = 40, LR_SCHEDULE = True
	Training [epoch: 1, batch: 100] loss: 0.510
Training [epoch: 1, batch: 200] loss: 0.491
Training [epoch: 1, batch: 300] loss: 0.479
Training [epoch: 1, batch: 400] loss: 0.474
Training [epoch: 1, batch: 500] loss: 0.481
Training [epoch: 1, batch: 600] loss: 0.478
Training [epoch: 1, batch: 700] loss: 0.473
Training [epoch: 1, batch: 800] loss: 0.470
Training [epoch: 1, batch: 900] loss: 0.465
Training [epoch: 1, batch: 1000] loss: 0.457
Training [epoch: 1, batch: 1100] loss: 0.453
Training [epoch: 1, batch: 1200] loss: 0.447
Training [epoch: 1, batch: 1300] loss: 0.440
Training [epoch: 1, batch: 1400] loss: 0.434
Training [epoch: 1, batch: 1500] loss: 0.427
Training [epoch: 1, batch: 1600] loss: 0.424
Training [epoch: 1, batch: 1700] loss: 0.421
Training [epoch: 1, batch: 1800] loss: 0.417
Training [epoch: 1, batch: 1900] loss: 0.412
Training [epoch: 1, batch: 2000] loss: 0.408
Training [epoch: 1, batch: 2100] loss: 0.404
Training [epoch: 1, batch: 2200] loss: 0.399
Training [epoch: 1, batch: 2300] loss: 0.394
Training [epoch: 1, batch: 2400] loss: 0.390
Training [epoch: 1, batch: 2500] loss: 0.387
Training [epoch: 1, batch: 2600] loss: 0.383
Training [epoch: 1, batch: 2700] loss: 0.381
Training [epoch: 1, batch: 2800] loss: 0.377
Training [epoch: 1, batch: 2900] loss: 0.375
Training [epoch: 1, batch: 3000] loss: 0.372
Training [epoch: 1, batch: 3100] loss: 0.369
Training [epoch: 1, batch: 3200] loss: 0.365
Training [epoch: 1, batch: 3300] loss: 0.362
Training [epoch: 1, batch: 3400] loss: 0.359
Training [epoch: 1, batch: 3500] loss: 0.357
Training [epoch: 1, batch: 3600] loss: 0.355
Training [epoch: 1, batch: 3700] loss: 0.353
Training [epoch: 1, batch: 3800] loss: 0.350
Training [epoch: 1, batch: 3900] loss: 0.348
Training [epoch: 1, batch: 4000] loss: 0.346
Training [epoch: 1, batch: 4100] loss: 0.344
Training [epoch: 1, batch: 4200] loss: 0.341
Training [epoch: 1, batch: 4300] loss: 0.339
Training [epoch: 1, batch: 4400] loss: 0.337
Training [epoch: 1, batch: 4500] loss: 0.335
Training [epoch: 1, batch: 4600] loss: 0.333
Training [epoch: 1, batch: 4700] loss: 0.332
Training [epoch: 1, batch: 4800] loss: 0.331
Training [epoch: 1, batch: 4900] loss: 0.329
Training [epoch: 1, batch: 5000] loss: 0.327
Training [epoch: 1, batch: 5100] loss: 0.325
Training [epoch: 1, batch: 5200] loss: 0.323
Training [epoch: 1, batch: 5300] loss: 0.322
Training [epoch: 1, batch: 5400] loss: 0.320
Training [epoch: 1, batch: 5500] loss: 0.318
Training [epoch: 1, batch: 5600] loss: 0.317
Training [epoch: 1, batch: 5700] loss: 0.315
Training [epoch: 1, batch: 5800] loss: 0.314
Training [epoch: 1, batch: 5900] loss: 0.312
Training [epoch: 1, batch: 6000] loss: 0.311
Training [epoch: 1, batch: 6100] loss: 0.310
Training [epoch: 1, batch: 6200] loss: 0.308
--- 8432.859091758728 seconds for 1 epoch ---
Training [epoch: 2, batch: 100] loss: 0.251
Training [epoch: 2, batch: 200] loss: 0.225
Training [epoch: 2, batch: 300] loss: 0.235
Training [epoch: 2, batch: 400] loss: 0.238
Training [epoch: 2, batch: 500] loss: 0.236
Training [epoch: 2, batch: 600] loss: 0.233
Training [epoch: 2, batch: 700] loss: 0.233
Training [epoch: 2, batch: 800] loss: 0.229
Training [epoch: 2, batch: 900] loss: 0.227
Training [epoch: 2, batch: 1000] loss: 0.224
Training [epoch: 2, batch: 1100] loss: 0.223
Training [epoch: 2, batch: 1200] loss: 0.221
Training [epoch: 2, batch: 1300] loss: 0.220
Training [epoch: 2, batch: 1400] loss: 0.219
Training [epoch: 2, batch: 1500] loss: 0.219
Training [epoch: 2, batch: 1600] loss: 0.219
Training [epoch: 2, batch: 1700] loss: 0.219
Training [epoch: 2, batch: 1800] loss: 0.220
Training [epoch: 2, batch: 1900] loss: 0.219
Training [epoch: 2, batch: 2000] loss: 0.219
Training [epoch: 2, batch: 2100] loss: 0.219
Training [epoch: 2, batch: 2200] loss: 0.219
Training [epoch: 2, batch: 2300] loss: 0.220
Training [epoch: 2, batch: 2400] loss: 0.221
Training [epoch: 2, batch: 2500] loss: 0.221
Training [epoch: 2, batch: 2600] loss: 0.220
Training [epoch: 2, batch: 2700] loss: 0.220
Training [epoch: 2, batch: 2800] loss: 0.220
Training [epoch: 2, batch: 2900] loss: 0.220
Training [epoch: 2, batch: 3000] loss: 0.221
Training [epoch: 2, batch: 3100] loss: 0.221
Training [epoch: 2, batch: 3200] loss: 0.220
Training [epoch: 2, batch: 3300] loss: 0.220
Training [epoch: 2, batch: 3400] loss: 0.219
Training [epoch: 2, batch: 3500] loss: 0.218
Training [epoch: 2, batch: 3600] loss: 0.217
Training [epoch: 2, batch: 3700] loss: 0.217
Training [epoch: 2, batch: 3800] loss: 0.216
Training [epoch: 2, batch: 3900] loss: 0.215
Training [epoch: 2, batch: 4000] loss: 0.214
Training [epoch: 2, batch: 4100] loss: 0.214
Training [epoch: 2, batch: 4200] loss: 0.213
Training [epoch: 2, batch: 4300] loss: 0.213
Training [epoch: 2, batch: 4400] loss: 0.212
Training [epoch: 2, batch: 4500] loss: 0.212
Training [epoch: 2, batch: 4600] loss: 0.211
Training [epoch: 2, batch: 4700] loss: 0.210
Training [epoch: 2, batch: 4800] loss: 0.210
Training [epoch: 2, batch: 4900] loss: 0.210
Training [epoch: 2, batch: 5000] loss: 0.209
Training [epoch: 2, batch: 5100] loss: 0.209
Training [epoch: 2, batch: 5200] loss: 0.208
Training [epoch: 2, batch: 5300] loss: 0.208
Training [epoch: 2, batch: 5400] loss: 0.208
Training [epoch: 2, batch: 5500] loss: 0.207
Training [epoch: 2, batch: 5600] loss: 0.207
Training [epoch: 2, batch: 5700] loss: 0.207
Training [epoch: 2, batch: 5800] loss: 0.207
Training [epoch: 2, batch: 5900] loss: 0.206
Training [epoch: 2, batch: 6000] loss: 0.206
Training [epoch: 2, batch: 6100] loss: 0.205
Training [epoch: 2, batch: 6200] loss: 0.204
--- 7097.329494714737 seconds for 1 epoch ---
Training [epoch: 3, batch: 100] loss: 0.193
Training [epoch: 3, batch: 200] loss: 0.190
Training [epoch: 3, batch: 300] loss: 0.195
Training [epoch: 3, batch: 400] loss: 0.193
Training [epoch: 3, batch: 500] loss: 0.192
Training [epoch: 3, batch: 600] loss: 0.190
Training [epoch: 3, batch: 700] loss: 0.186
Training [epoch: 3, batch: 800] loss: 0.184
Training [epoch: 3, batch: 900] loss: 0.185
Training [epoch: 3, batch: 1000] loss: 0.184
Training [epoch: 3, batch: 1100] loss: 0.183
Training [epoch: 3, batch: 1200] loss: 0.181
Training [epoch: 3, batch: 1300] loss: 0.180
Training [epoch: 3, batch: 1400] loss: 0.179
Training [epoch: 3, batch: 1500] loss: 0.180
Training [epoch: 3, batch: 1600] loss: 0.179
Training [epoch: 3, batch: 1700] loss: 0.179
Training [epoch: 3, batch: 1800] loss: 0.180
Training [epoch: 3, batch: 1900] loss: 0.180
Training [epoch: 3, batch: 2000] loss: 0.179
Training [epoch: 3, batch: 2100] loss: 0.179
Training [epoch: 3, batch: 2200] loss: 0.179
Training [epoch: 3, batch: 2300] loss: 0.180
Training [epoch: 3, batch: 2400] loss: 0.179
Training [epoch: 3, batch: 2500] loss: 0.180
Training [epoch: 3, batch: 2600] loss: 0.180
Training [epoch: 3, batch: 2700] loss: 0.181
Training [epoch: 3, batch: 2800] loss: 0.181
Training [epoch: 3, batch: 2900] loss: 0.181
Training [epoch: 3, batch: 3000] loss: 0.181
Training [epoch: 3, batch: 3100] loss: 0.182
Training [epoch: 3, batch: 3200] loss: 0.182
Training [epoch: 3, batch: 3300] loss: 0.182
Training [epoch: 3, batch: 3400] loss: 0.181
Training [epoch: 3, batch: 3500] loss: 0.180
Training [epoch: 3, batch: 3600] loss: 0.180
Training [epoch: 3, batch: 3700] loss: 0.181
Training [epoch: 3, batch: 3800] loss: 0.180
Training [epoch: 3, batch: 3900] loss: 0.180
Training [epoch: 3, batch: 4000] loss: 0.180
Training [epoch: 3, batch: 4100] loss: 0.180
Training [epoch: 3, batch: 4200] loss: 0.180
Training [epoch: 3, batch: 4300] loss: 0.180
Training [epoch: 3, batch: 4400] loss: 0.179
Training [epoch: 3, batch: 4500] loss: 0.179
Training [epoch: 3, batch: 4600] loss: 0.178
Training [epoch: 3, batch: 4700] loss: 0.178
Training [epoch: 3, batch: 4800] loss: 0.178
Training [epoch: 3, batch: 4900] loss: 0.177
Training [epoch: 3, batch: 5000] loss: 0.177
Training [epoch: 3, batch: 5100] loss: 0.177
Training [epoch: 3, batch: 5200] loss: 0.177
Training [epoch: 3, batch: 5300] loss: 0.177
Training [epoch: 3, batch: 5400] loss: 0.177
Training [epoch: 3, batch: 5500] loss: 0.176
Training [epoch: 3, batch: 5600] loss: 0.176
Training [epoch: 3, batch: 5700] loss: 0.177
Training [epoch: 3, batch: 5800] loss: 0.177
Training [epoch: 3, batch: 5900] loss: 0.176
Training [epoch: 3, batch: 6000] loss: 0.176
Training [epoch: 3, batch: 6100] loss: 0.176
Training [epoch: 3, batch: 6200] loss: 0.176
--- 7091.621897220612 seconds for 1 epoch ---
Training [epoch: 4, batch: 100] loss: 0.155
Training [epoch: 4, batch: 200] loss: 0.152
Training [epoch: 4, batch: 300] loss: 0.150
Training [epoch: 4, batch: 400] loss: 0.150
Training [epoch: 4, batch: 500] loss: 0.150
Training [epoch: 4, batch: 600] loss: 0.155
Training [epoch: 4, batch: 700] loss: 0.157
Training [epoch: 4, batch: 800] loss: 0.157
Training [epoch: 4, batch: 900] loss: 0.157
Training [epoch: 4, batch: 1000] loss: 0.159
Training [epoch: 4, batch: 1100] loss: 0.157
Training [epoch: 4, batch: 1200] loss: 0.158
Training [epoch: 4, batch: 1300] loss: 0.156
Training [epoch: 4, batch: 1400] loss: 0.157
Training [epoch: 4, batch: 1500] loss: 0.158
Training [epoch: 4, batch: 1600] loss: 0.158
Training [epoch: 4, batch: 1700] loss: 0.158
Training [epoch: 4, batch: 1800] loss: 0.157
Training [epoch: 4, batch: 1900] loss: 0.157
Training [epoch: 4, batch: 2000] loss: 0.158
Training [epoch: 4, batch: 2100] loss: 0.159
Training [epoch: 4, batch: 2200] loss: 0.160
Training [epoch: 4, batch: 2300] loss: 0.159
Training [epoch: 4, batch: 2400] loss: 0.159
Training [epoch: 4, batch: 2500] loss: 0.158
Training [epoch: 4, batch: 2600] loss: 0.158
Training [epoch: 4, batch: 2700] loss: 0.158
Training [epoch: 4, batch: 2800] loss: 0.157
Training [epoch: 4, batch: 2900] loss: 0.156
Training [epoch: 4, batch: 3000] loss: 0.156
Training [epoch: 4, batch: 3100] loss: 0.156
Training [epoch: 4, batch: 3200] loss: 0.157
Training [epoch: 4, batch: 3300] loss: 0.157
Training [epoch: 4, batch: 3400] loss: 0.157
Training [epoch: 4, batch: 3500] loss: 0.157
Training [epoch: 4, batch: 3600] loss: 0.157
Training [epoch: 4, batch: 3700] loss: 0.157
Training [epoch: 4, batch: 3800] loss: 0.157
Training [epoch: 4, batch: 3900] loss: 0.156
Training [epoch: 4, batch: 4000] loss: 0.156
Training [epoch: 4, batch: 4100] loss: 0.156
Training [epoch: 4, batch: 4200] loss: 0.156
Training [epoch: 4, batch: 4300] loss: 0.157
Training [epoch: 4, batch: 4400] loss: 0.156
Training [epoch: 4, batch: 4500] loss: 0.156
Training [epoch: 4, batch: 4600] loss: 0.156
Training [epoch: 4, batch: 4700] loss: 0.156
Training [epoch: 4, batch: 4800] loss: 0.157
Training [epoch: 4, batch: 4900] loss: 0.157
Training [epoch: 4, batch: 5000] loss: 0.156
Training [epoch: 4, batch: 5100] loss: 0.156
Training [epoch: 4, batch: 5200] loss: 0.156
Training [epoch: 4, batch: 5300] loss: 0.156
Training [epoch: 4, batch: 5400] loss: 0.156
Training [epoch: 4, batch: 5500] loss: 0.156
Training [epoch: 4, batch: 5600] loss: 0.156
Training [epoch: 4, batch: 5700] loss: 0.156
Training [epoch: 4, batch: 5800] loss: 0.156
Training [epoch: 4, batch: 5900] loss: 0.156
Training [epoch: 4, batch: 6000] loss: 0.156
Training [epoch: 4, batch: 6100] loss: 0.156
Training [epoch: 4, batch: 6200] loss: 0.156
--- 7094.697685718536 seconds for 1 epoch ---
Training [epoch: 5, batch: 100] loss: 0.153
Training [epoch: 5, batch: 200] loss: 0.159
Training [epoch: 5, batch: 300] loss: 0.156
Training [epoch: 5, batch: 400] loss: 0.153
Training [epoch: 5, batch: 500] loss: 0.155
Training [epoch: 5, batch: 600] loss: 0.152
Training [epoch: 5, batch: 700] loss: 0.149
Training [epoch: 5, batch: 800] loss: 0.149
Training [epoch: 5, batch: 900] loss: 0.150
Training [epoch: 5, batch: 1000] loss: 0.149
Training [epoch: 5, batch: 1100] loss: 0.151
Training [epoch: 5, batch: 1200] loss: 0.150
Training [epoch: 5, batch: 1300] loss: 0.149
Training [epoch: 5, batch: 1400] loss: 0.148
Training [epoch: 5, batch: 1500] loss: 0.147
Training [epoch: 5, batch: 1600] loss: 0.147
Training [epoch: 5, batch: 1700] loss: 0.146
Training [epoch: 5, batch: 1800] loss: 0.146
Training [epoch: 5, batch: 1900] loss: 0.146
Training [epoch: 5, batch: 2000] loss: 0.147
Training [epoch: 5, batch: 2100] loss: 0.146
Training [epoch: 5, batch: 2200] loss: 0.147
Training [epoch: 5, batch: 2300] loss: 0.148
Training [epoch: 5, batch: 2400] loss: 0.148
Training [epoch: 5, batch: 2500] loss: 0.148
Training [epoch: 5, batch: 2600] loss: 0.147
Training [epoch: 5, batch: 2700] loss: 0.147
Training [epoch: 5, batch: 2800] loss: 0.146
Training [epoch: 5, batch: 2900] loss: 0.146
Training [epoch: 5, batch: 3000] loss: 0.147
Training [epoch: 5, batch: 3100] loss: 0.148
Training [epoch: 5, batch: 3200] loss: 0.149
Training [epoch: 5, batch: 3300] loss: 0.148
Training [epoch: 5, batch: 3400] loss: 0.148
Training [epoch: 5, batch: 3500] loss: 0.148
Training [epoch: 5, batch: 3600] loss: 0.149
Training [epoch: 5, batch: 3700] loss: 0.149
Training [epoch: 5, batch: 3800] loss: 0.148
Training [epoch: 5, batch: 3900] loss: 0.148
Training [epoch: 5, batch: 4000] loss: 0.148
Training [epoch: 5, batch: 4100] loss: 0.148
Training [epoch: 5, batch: 4200] loss: 0.148
Training [epoch: 5, batch: 4300] loss: 0.148
Training [epoch: 5, batch: 4400] loss: 0.148
Training [epoch: 5, batch: 4500] loss: 0.148
Training [epoch: 5, batch: 4600] loss: 0.147
Training [epoch: 5, batch: 4700] loss: 0.147
Training [epoch: 5, batch: 4800] loss: 0.147
Training [epoch: 5, batch: 4900] loss: 0.147
Training [epoch: 5, batch: 5000] loss: 0.147
Training [epoch: 5, batch: 5100] loss: 0.147
Training [epoch: 5, batch: 5200] loss: 0.147
Training [epoch: 5, batch: 5300] loss: 0.147
Training [epoch: 5, batch: 5400] loss: 0.147
Training [epoch: 5, batch: 5500] loss: 0.147
Training [epoch: 5, batch: 5600] loss: 0.147
Training [epoch: 5, batch: 5700] loss: 0.147
Training [epoch: 5, batch: 5800] loss: 0.146
Training [epoch: 5, batch: 5900] loss: 0.146
Training [epoch: 5, batch: 6000] loss: 0.146
Training [epoch: 5, batch: 6100] loss: 0.146
Training [epoch: 5, batch: 6200] loss: 0.146
--- 7137.429759025574 seconds for 1 epoch ---
Training [epoch: 6, batch: 100] loss: 0.134
Training [epoch: 6, batch: 200] loss: 0.137
Training [epoch: 6, batch: 300] loss: 0.131
Training [epoch: 6, batch: 400] loss: 0.130
Training [epoch: 6, batch: 500] loss: 0.130
Training [epoch: 6, batch: 600] loss: 0.129
Training [epoch: 6, batch: 700] loss: 0.132
Training [epoch: 6, batch: 800] loss: 0.131
Training [epoch: 6, batch: 900] loss: 0.130
Training [epoch: 6, batch: 1000] loss: 0.129
Training [epoch: 6, batch: 1100] loss: 0.132
Training [epoch: 6, batch: 1200] loss: 0.132
Training [epoch: 6, batch: 1300] loss: 0.131
Training [epoch: 6, batch: 1400] loss: 0.132
Training [epoch: 6, batch: 1500] loss: 0.133
Training [epoch: 6, batch: 1600] loss: 0.133
Training [epoch: 6, batch: 1700] loss: 0.133
Training [epoch: 6, batch: 1800] loss: 0.132
Training [epoch: 6, batch: 1900] loss: 0.131
Training [epoch: 6, batch: 2000] loss: 0.131
Training [epoch: 6, batch: 2100] loss: 0.130
Training [epoch: 6, batch: 2200] loss: 0.130
Training [epoch: 6, batch: 2300] loss: 0.130
Training [epoch: 6, batch: 2400] loss: 0.130
Training [epoch: 6, batch: 2500] loss: 0.130
Training [epoch: 6, batch: 2600] loss: 0.130
Training [epoch: 6, batch: 2700] loss: 0.130
Training [epoch: 6, batch: 2800] loss: 0.130
Training [epoch: 6, batch: 2900] loss: 0.130
Training [epoch: 6, batch: 3000] loss: 0.130
Training [epoch: 6, batch: 3100] loss: 0.130
Training [epoch: 6, batch: 3200] loss: 0.131
Training [epoch: 6, batch: 3300] loss: 0.131
Training [epoch: 6, batch: 3400] loss: 0.132
Training [epoch: 6, batch: 3500] loss: 0.131
Training [epoch: 6, batch: 3600] loss: 0.132
Training [epoch: 6, batch: 3700] loss: 0.132
Training [epoch: 6, batch: 3800] loss: 0.132
Training [epoch: 6, batch: 3900] loss: 0.132
Training [epoch: 6, batch: 4000] loss: 0.132
Training [epoch: 6, batch: 4100] loss: 0.132
Training [epoch: 6, batch: 4200] loss: 0.131
Training [epoch: 6, batch: 4300] loss: 0.131
Training [epoch: 6, batch: 4400] loss: 0.131
Training [epoch: 6, batch: 4500] loss: 0.131
Training [epoch: 6, batch: 4600] loss: 0.131
Training [epoch: 6, batch: 4700] loss: 0.131
Training [epoch: 6, batch: 4800] loss: 0.131
Training [epoch: 6, batch: 4900] loss: 0.131
Training [epoch: 6, batch: 5000] loss: 0.131
Training [epoch: 6, batch: 5100] loss: 0.131
Training [epoch: 6, batch: 5200] loss: 0.131
Training [epoch: 6, batch: 5300] loss: 0.131
Training [epoch: 6, batch: 5400] loss: 0.130
Training [epoch: 6, batch: 5500] loss: 0.130
Training [epoch: 6, batch: 5600] loss: 0.130
Training [epoch: 6, batch: 5700] loss: 0.130
Training [epoch: 6, batch: 5800] loss: 0.130
Training [epoch: 6, batch: 5900] loss: 0.130
Training [epoch: 6, batch: 6000] loss: 0.130
Training [epoch: 6, batch: 6100] loss: 0.129
Training [epoch: 6, batch: 6200] loss: 0.129
--- 7098.4138696193695 seconds for 1 epoch ---
Training [epoch: 7, batch: 100] loss: 0.125
Training [epoch: 7, batch: 200] loss: 0.117
Training [epoch: 7, batch: 300] loss: 0.127
Training [epoch: 7, batch: 400] loss: 0.126
Training [epoch: 7, batch: 500] loss: 0.124
Training [epoch: 7, batch: 600] loss: 0.122
Training [epoch: 7, batch: 700] loss: 0.121
Training [epoch: 7, batch: 800] loss: 0.121
Training [epoch: 7, batch: 900] loss: 0.122
Training [epoch: 7, batch: 1000] loss: 0.121
Training [epoch: 7, batch: 1100] loss: 0.119
Training [epoch: 7, batch: 1200] loss: 0.119
Training [epoch: 7, batch: 1300] loss: 0.119
Training [epoch: 7, batch: 1400] loss: 0.120
Training [epoch: 7, batch: 1500] loss: 0.118
Training [epoch: 7, batch: 1600] loss: 0.119
Training [epoch: 7, batch: 1700] loss: 0.118
Training [epoch: 7, batch: 1800] loss: 0.118
Training [epoch: 7, batch: 1900] loss: 0.118
Training [epoch: 7, batch: 2000] loss: 0.118
Training [epoch: 7, batch: 2100] loss: 0.119
Training [epoch: 7, batch: 2200] loss: 0.118
Training [epoch: 7, batch: 2300] loss: 0.118
Training [epoch: 7, batch: 2400] loss: 0.117
Training [epoch: 7, batch: 2500] loss: 0.117
Training [epoch: 7, batch: 2600] loss: 0.117
Training [epoch: 7, batch: 2700] loss: 0.117
Training [epoch: 7, batch: 2800] loss: 0.117
Training [epoch: 7, batch: 2900] loss: 0.117
Training [epoch: 7, batch: 3000] loss: 0.116
Training [epoch: 7, batch: 3100] loss: 0.117
Training [epoch: 7, batch: 3200] loss: 0.117
Training [epoch: 7, batch: 3300] loss: 0.118
Training [epoch: 7, batch: 3400] loss: 0.118
Training [epoch: 7, batch: 3500] loss: 0.118
Training [epoch: 7, batch: 3600] loss: 0.118
Training [epoch: 7, batch: 3700] loss: 0.118
Training [epoch: 7, batch: 3800] loss: 0.118
Training [epoch: 7, batch: 3900] loss: 0.118
Training [epoch: 7, batch: 4000] loss: 0.118
Training [epoch: 7, batch: 4100] loss: 0.118
Training [epoch: 7, batch: 4200] loss: 0.118
Training [epoch: 7, batch: 4300] loss: 0.118
Training [epoch: 7, batch: 4400] loss: 0.118
Training [epoch: 7, batch: 4500] loss: 0.118
Training [epoch: 7, batch: 4600] loss: 0.118
Training [epoch: 7, batch: 4700] loss: 0.118
Training [epoch: 7, batch: 4800] loss: 0.117
Training [epoch: 7, batch: 4900] loss: 0.117
Training [epoch: 7, batch: 5000] loss: 0.117
Training [epoch: 7, batch: 5100] loss: 0.117
Training [epoch: 7, batch: 5200] loss: 0.117
Training [epoch: 7, batch: 5300] loss: 0.117
Training [epoch: 7, batch: 5400] loss: 0.118
Training [epoch: 7, batch: 5500] loss: 0.118
Training [epoch: 7, batch: 5600] loss: 0.118
Training [epoch: 7, batch: 5700] loss: 0.118
Training [epoch: 7, batch: 5800] loss: 0.118
Training [epoch: 7, batch: 5900] loss: 0.117
Training [epoch: 7, batch: 6000] loss: 0.117
Training [epoch: 7, batch: 6100] loss: 0.117
Training [epoch: 7, batch: 6200] loss: 0.117
--- 7097.129670143127 seconds for 1 epoch ---
Training [epoch: 8, batch: 100] loss: 0.108
Training [epoch: 8, batch: 200] loss: 0.105
Training [epoch: 8, batch: 300] loss: 0.115
Training [epoch: 8, batch: 400] loss: 0.110
Training [epoch: 8, batch: 500] loss: 0.115
Training [epoch: 8, batch: 600] loss: 0.115
Training [epoch: 8, batch: 700] loss: 0.115
Training [epoch: 8, batch: 800] loss: 0.115
Training [epoch: 8, batch: 900] loss: 0.116
Training [epoch: 8, batch: 1000] loss: 0.117
Training [epoch: 8, batch: 1100] loss: 0.115
Training [epoch: 8, batch: 1200] loss: 0.116
Training [epoch: 8, batch: 1300] loss: 0.114
Training [epoch: 8, batch: 1400] loss: 0.114
Training [epoch: 8, batch: 1500] loss: 0.113
Training [epoch: 8, batch: 1600] loss: 0.114
Training [epoch: 8, batch: 1700] loss: 0.115
Training [epoch: 8, batch: 1800] loss: 0.115
Training [epoch: 8, batch: 1900] loss: 0.114
Training [epoch: 8, batch: 2000] loss: 0.114
Training [epoch: 8, batch: 2100] loss: 0.114
Training [epoch: 8, batch: 2200] loss: 0.113
Training [epoch: 8, batch: 2300] loss: 0.112
Training [epoch: 8, batch: 2400] loss: 0.112
Training [epoch: 8, batch: 2500] loss: 0.111
Training [epoch: 8, batch: 2600] loss: 0.111
Training [epoch: 8, batch: 2700] loss: 0.112
Training [epoch: 8, batch: 2800] loss: 0.111
Training [epoch: 8, batch: 2900] loss: 0.111
Training [epoch: 8, batch: 3000] loss: 0.111
Training [epoch: 8, batch: 3100] loss: 0.110
Training [epoch: 8, batch: 3200] loss: 0.111
Training [epoch: 8, batch: 3300] loss: 0.111
Training [epoch: 8, batch: 3400] loss: 0.111
Training [epoch: 8, batch: 3500] loss: 0.111
Training [epoch: 8, batch: 3600] loss: 0.111
Training [epoch: 8, batch: 3700] loss: 0.110
Training [epoch: 8, batch: 3800] loss: 0.110
Training [epoch: 8, batch: 3900] loss: 0.111
Training [epoch: 8, batch: 4000] loss: 0.110
Training [epoch: 8, batch: 4100] loss: 0.110
Training [epoch: 8, batch: 4200] loss: 0.110
Training [epoch: 8, batch: 4300] loss: 0.109
Training [epoch: 8, batch: 4400] loss: 0.109
Training [epoch: 8, batch: 4500] loss: 0.110
Training [epoch: 8, batch: 4600] loss: 0.109
Training [epoch: 8, batch: 4700] loss: 0.109
Training [epoch: 8, batch: 4800] loss: 0.109
Training [epoch: 8, batch: 4900] loss: 0.109
Training [epoch: 8, batch: 5000] loss: 0.109
Training [epoch: 8, batch: 5100] loss: 0.109
Training [epoch: 8, batch: 5200] loss: 0.109
Training [epoch: 8, batch: 5300] loss: 0.109
Training [epoch: 8, batch: 5400] loss: 0.109
Training [epoch: 8, batch: 5500] loss: 0.109
Training [epoch: 8, batch: 5600] loss: 0.109
Training [epoch: 8, batch: 5700] loss: 0.109
Training [epoch: 8, batch: 5800] loss: 0.109
Training [epoch: 8, batch: 5900] loss: 0.109
Training [epoch: 8, batch: 6000] loss: 0.109
Training [epoch: 8, batch: 6100] loss: 0.109
Training [epoch: 8, batch: 6200] loss: 0.109
--- 7108.213738918304 seconds for 1 epoch ---
Training [epoch: 9, batch: 100] loss: 0.095
Training [epoch: 9, batch: 200] loss: 0.105
Training [epoch: 9, batch: 300] loss: 0.104
Training [epoch: 9, batch: 400] loss: 0.103
Training [epoch: 9, batch: 500] loss: 0.105
Training [epoch: 9, batch: 600] loss: 0.103
Training [epoch: 9, batch: 700] loss: 0.101
Training [epoch: 9, batch: 800] loss: 0.099
Training [epoch: 9, batch: 900] loss: 0.099
Training [epoch: 9, batch: 1000] loss: 0.100
Training [epoch: 9, batch: 1100] loss: 0.101
Training [epoch: 9, batch: 1200] loss: 0.100
Training [epoch: 9, batch: 1300] loss: 0.100
Training [epoch: 9, batch: 1400] loss: 0.100
Training [epoch: 9, batch: 1500] loss: 0.100
Training [epoch: 9, batch: 1600] loss: 0.101
Training [epoch: 9, batch: 1700] loss: 0.101
Training [epoch: 9, batch: 1800] loss: 0.102
Training [epoch: 9, batch: 1900] loss: 0.102
Training [epoch: 9, batch: 2000] loss: 0.102
Training [epoch: 9, batch: 2100] loss: 0.101
Training [epoch: 9, batch: 2200] loss: 0.101
Training [epoch: 9, batch: 2300] loss: 0.101
Training [epoch: 9, batch: 2400] loss: 0.102
Training [epoch: 9, batch: 2500] loss: 0.102
Training [epoch: 9, batch: 2600] loss: 0.102
Training [epoch: 9, batch: 2700] loss: 0.103
Training [epoch: 9, batch: 2800] loss: 0.102
Training [epoch: 9, batch: 2900] loss: 0.101
Training [epoch: 9, batch: 3000] loss: 0.100
Training [epoch: 9, batch: 3100] loss: 0.100
Training [epoch: 9, batch: 3200] loss: 0.100
Training [epoch: 9, batch: 3300] loss: 0.100
Training [epoch: 9, batch: 3400] loss: 0.099
Training [epoch: 9, batch: 3500] loss: 0.099
Training [epoch: 9, batch: 3600] loss: 0.099
Training [epoch: 9, batch: 3700] loss: 0.099
Training [epoch: 9, batch: 3800] loss: 0.099
Training [epoch: 9, batch: 3900] loss: 0.099
Training [epoch: 9, batch: 4000] loss: 0.099
Training [epoch: 9, batch: 4100] loss: 0.098
Training [epoch: 9, batch: 4200] loss: 0.099
Training [epoch: 9, batch: 4300] loss: 0.099
Training [epoch: 9, batch: 4400] loss: 0.099
Training [epoch: 9, batch: 4500] loss: 0.099
Training [epoch: 9, batch: 4600] loss: 0.099
Training [epoch: 9, batch: 4700] loss: 0.099
Training [epoch: 9, batch: 4800] loss: 0.099
Training [epoch: 9, batch: 4900] loss: 0.099
Training [epoch: 9, batch: 5000] loss: 0.098
Training [epoch: 9, batch: 5100] loss: 0.098
Training [epoch: 9, batch: 5200] loss: 0.098
Training [epoch: 9, batch: 5300] loss: 0.098
Training [epoch: 9, batch: 5400] loss: 0.099
Training [epoch: 9, batch: 5500] loss: 0.099
Training [epoch: 9, batch: 5600] loss: 0.099
Training [epoch: 9, batch: 5700] loss: 0.099
Training [epoch: 9, batch: 5800] loss: 0.099
Training [epoch: 9, batch: 5900] loss: 0.099
Training [epoch: 9, batch: 6000] loss: 0.099
Training [epoch: 9, batch: 6100] loss: 0.100
Training [epoch: 9, batch: 6200] loss: 0.099
--- 7105.443053483963 seconds for 1 epoch ---
Training [epoch: 10, batch: 100] loss: 0.093
Training [epoch: 10, batch: 200] loss: 0.092
Training [epoch: 10, batch: 300] loss: 0.089
Training [epoch: 10, batch: 400] loss: 0.092
Training [epoch: 10, batch: 500] loss: 0.092
Training [epoch: 10, batch: 600] loss: 0.095
Training [epoch: 10, batch: 700] loss: 0.094
Training [epoch: 10, batch: 800] loss: 0.093
Training [epoch: 10, batch: 900] loss: 0.092
Training [epoch: 10, batch: 1000] loss: 0.092
Training [epoch: 10, batch: 1100] loss: 0.091
Training [epoch: 10, batch: 1200] loss: 0.092
Training [epoch: 10, batch: 1300] loss: 0.091
Training [epoch: 10, batch: 1400] loss: 0.091
Training [epoch: 10, batch: 1500] loss: 0.091
Training [epoch: 10, batch: 1600] loss: 0.091
Training [epoch: 10, batch: 1700] loss: 0.091
Training [epoch: 10, batch: 1800] loss: 0.091
Training [epoch: 10, batch: 1900] loss: 0.091
Training [epoch: 10, batch: 2000] loss: 0.091
Training [epoch: 10, batch: 2100] loss: 0.091
Training [epoch: 10, batch: 2200] loss: 0.091
Training [epoch: 10, batch: 2300] loss: 0.091
Training [epoch: 10, batch: 2400] loss: 0.091
Training [epoch: 10, batch: 2500] loss: 0.091
Training [epoch: 10, batch: 2600] loss: 0.090
Training [epoch: 10, batch: 2700] loss: 0.090
Training [epoch: 10, batch: 2800] loss: 0.090
Training [epoch: 10, batch: 2900] loss: 0.090
Training [epoch: 10, batch: 3000] loss: 0.090
Training [epoch: 10, batch: 3100] loss: 0.090
Training [epoch: 10, batch: 3200] loss: 0.090
Training [epoch: 10, batch: 3300] loss: 0.089
Training [epoch: 10, batch: 3400] loss: 0.089
Training [epoch: 10, batch: 3500] loss: 0.089
Training [epoch: 10, batch: 3600] loss: 0.088
Training [epoch: 10, batch: 3700] loss: 0.088
Training [epoch: 10, batch: 3800] loss: 0.088
Training [epoch: 10, batch: 3900] loss: 0.088
Training [epoch: 10, batch: 4000] loss: 0.088
Training [epoch: 10, batch: 4100] loss: 0.088
Training [epoch: 10, batch: 4200] loss: 0.089
Training [epoch: 10, batch: 4300] loss: 0.088
Training [epoch: 10, batch: 4400] loss: 0.088
Training [epoch: 10, batch: 4500] loss: 0.089
Training [epoch: 10, batch: 4600] loss: 0.089
Training [epoch: 10, batch: 4700] loss: 0.089
Training [epoch: 10, batch: 4800] loss: 0.088
Training [epoch: 10, batch: 4900] loss: 0.088
Training [epoch: 10, batch: 5000] loss: 0.088
Training [epoch: 10, batch: 5100] loss: 0.088
Training [epoch: 10, batch: 5200] loss: 0.087
Training [epoch: 10, batch: 5300] loss: 0.087
Training [epoch: 10, batch: 5400] loss: 0.087
Training [epoch: 10, batch: 5500] loss: 0.087
Training [epoch: 10, batch: 5600] loss: 0.087
Training [epoch: 10, batch: 5700] loss: 0.087
Training [epoch: 10, batch: 5800] loss: 0.087
Training [epoch: 10, batch: 5900] loss: 0.087
Training [epoch: 10, batch: 6000] loss: 0.087
Training [epoch: 10, batch: 6100] loss: 0.088
Training [epoch: 10, batch: 6200] loss: 0.087
--- 7148.281438589096 seconds for 1 epoch ---
Training [epoch: 11, batch: 100] loss: 0.093
Training [epoch: 11, batch: 200] loss: 0.085
Training [epoch: 11, batch: 300] loss: 0.086
Training [epoch: 11, batch: 400] loss: 0.085
Training [epoch: 11, batch: 500] loss: 0.085
Training [epoch: 11, batch: 600] loss: 0.087
Training [epoch: 11, batch: 700] loss: 0.086
Training [epoch: 11, batch: 800] loss: 0.087
Training [epoch: 11, batch: 900] loss: 0.085
Training [epoch: 11, batch: 1000] loss: 0.084
Training [epoch: 11, batch: 1100] loss: 0.083
Training [epoch: 11, batch: 1200] loss: 0.082
Training [epoch: 11, batch: 1300] loss: 0.083
Training [epoch: 11, batch: 1400] loss: 0.082
Training [epoch: 11, batch: 1500] loss: 0.083
Training [epoch: 11, batch: 1600] loss: 0.083
Training [epoch: 11, batch: 1700] loss: 0.083
Training [epoch: 11, batch: 1800] loss: 0.083
Training [epoch: 11, batch: 1900] loss: 0.083
Training [epoch: 11, batch: 2000] loss: 0.082
Training [epoch: 11, batch: 2100] loss: 0.082
Training [epoch: 11, batch: 2200] loss: 0.081
Training [epoch: 11, batch: 2300] loss: 0.080
Training [epoch: 11, batch: 2400] loss: 0.080
Training [epoch: 11, batch: 2500] loss: 0.080
Training [epoch: 11, batch: 2600] loss: 0.079
Training [epoch: 11, batch: 2700] loss: 0.079
Training [epoch: 11, batch: 2800] loss: 0.079
Training [epoch: 11, batch: 2900] loss: 0.079
Training [epoch: 11, batch: 3000] loss: 0.079
Training [epoch: 11, batch: 3100] loss: 0.079
Training [epoch: 11, batch: 3200] loss: 0.079
Training [epoch: 11, batch: 3300] loss: 0.079
Training [epoch: 11, batch: 3400] loss: 0.079
Training [epoch: 11, batch: 3500] loss: 0.079
Training [epoch: 11, batch: 3600] loss: 0.078
Training [epoch: 11, batch: 3700] loss: 0.078
Training [epoch: 11, batch: 3800] loss: 0.078
Training [epoch: 11, batch: 3900] loss: 0.077
Training [epoch: 11, batch: 4000] loss: 0.078
Training [epoch: 11, batch: 4100] loss: 0.077
Training [epoch: 11, batch: 4200] loss: 0.077
Training [epoch: 11, batch: 4300] loss: 0.076
Training [epoch: 11, batch: 4400] loss: 0.076
Training [epoch: 11, batch: 4500] loss: 0.076
Training [epoch: 11, batch: 4600] loss: 0.076
Training [epoch: 11, batch: 4700] loss: 0.076
Training [epoch: 11, batch: 4800] loss: 0.076
Training [epoch: 11, batch: 4900] loss: 0.076
Training [epoch: 11, batch: 5000] loss: 0.075
Training [epoch: 11, batch: 5100] loss: 0.075
Training [epoch: 11, batch: 5200] loss: 0.075
Training [epoch: 11, batch: 5300] loss: 0.075
Training [epoch: 11, batch: 5400] loss: 0.075
Training [epoch: 11, batch: 5500] loss: 0.075
Training [epoch: 11, batch: 5600] loss: 0.075
Training [epoch: 11, batch: 5700] loss: 0.075
Training [epoch: 11, batch: 5800] loss: 0.075
Training [epoch: 11, batch: 5900] loss: 0.075
Training [epoch: 11, batch: 6000] loss: 0.075
Training [epoch: 11, batch: 6100] loss: 0.074
Training [epoch: 11, batch: 6200] loss: 0.074
--- 7152.077835083008 seconds for 1 epoch ---
Training [epoch: 12, batch: 100] loss: 0.053
Training [epoch: 12, batch: 200] loss: 0.055
Training [epoch: 12, batch: 300] loss: 0.060
Training [epoch: 12, batch: 400] loss: 0.057
Training [epoch: 12, batch: 500] loss: 0.060
Training [epoch: 12, batch: 600] loss: 0.061
Training [epoch: 12, batch: 700] loss: 0.061
Training [epoch: 12, batch: 800] loss: 0.062
Training [epoch: 12, batch: 900] loss: 0.061
Training [epoch: 12, batch: 1000] loss: 0.061
Training [epoch: 12, batch: 1100] loss: 0.062
Training [epoch: 12, batch: 1200] loss: 0.063
Training [epoch: 12, batch: 1300] loss: 0.062
Training [epoch: 12, batch: 1400] loss: 0.063
Training [epoch: 12, batch: 1500] loss: 0.063
Training [epoch: 12, batch: 1600] loss: 0.063
Training [epoch: 12, batch: 1700] loss: 0.063
Training [epoch: 12, batch: 1800] loss: 0.064
Training [epoch: 12, batch: 1900] loss: 0.064
Training [epoch: 12, batch: 2000] loss: 0.064
Training [epoch: 12, batch: 2100] loss: 0.065
Training [epoch: 12, batch: 2200] loss: 0.065
Training [epoch: 12, batch: 2300] loss: 0.065
Training [epoch: 12, batch: 2400] loss: 0.065
Training [epoch: 12, batch: 2500] loss: 0.064
Training [epoch: 12, batch: 2600] loss: 0.064
Training [epoch: 12, batch: 2700] loss: 0.064
Training [epoch: 12, batch: 2800] loss: 0.065
Training [epoch: 12, batch: 2900] loss: 0.065
Training [epoch: 12, batch: 3000] loss: 0.066
Training [epoch: 12, batch: 3100] loss: 0.066
Training [epoch: 12, batch: 3200] loss: 0.067
Training [epoch: 12, batch: 3300] loss: 0.066
Training [epoch: 12, batch: 3400] loss: 0.066
Training [epoch: 12, batch: 3500] loss: 0.066
Training [epoch: 12, batch: 3600] loss: 0.066
Training [epoch: 12, batch: 3700] loss: 0.066
Training [epoch: 12, batch: 3800] loss: 0.066
Training [epoch: 12, batch: 3900] loss: 0.066
Training [epoch: 12, batch: 4000] loss: 0.066
Training [epoch: 12, batch: 4100] loss: 0.066
Training [epoch: 12, batch: 4200] loss: 0.067
Training [epoch: 12, batch: 4300] loss: 0.066
Training [epoch: 12, batch: 4400] loss: 0.066
Training [epoch: 12, batch: 4500] loss: 0.066
Training [epoch: 12, batch: 4600] loss: 0.065
Training [epoch: 12, batch: 4700] loss: 0.065
Training [epoch: 12, batch: 4800] loss: 0.065
Training [epoch: 12, batch: 4900] loss: 0.065
Training [epoch: 12, batch: 5000] loss: 0.065
Training [epoch: 12, batch: 5100] loss: 0.065
Training [epoch: 12, batch: 5200] loss: 0.065
Training [epoch: 12, batch: 5300] loss: 0.065
Training [epoch: 12, batch: 5400] loss: 0.065
Training [epoch: 12, batch: 5500] loss: 0.065
Training [epoch: 12, batch: 5600] loss: 0.065
Training [epoch: 12, batch: 5700] loss: 0.064
Training [epoch: 12, batch: 5800] loss: 0.064
Training [epoch: 12, batch: 5900] loss: 0.064
Training [epoch: 12, batch: 6000] loss: 0.064
Training [epoch: 12, batch: 6100] loss: 0.064
Training [epoch: 12, batch: 6200] loss: 0.064
--- 7142.717837572098 seconds for 1 epoch ---
Training [epoch: 13, batch: 100] loss: 0.054
Training [epoch: 13, batch: 200] loss: 0.053
Training [epoch: 13, batch: 300] loss: 0.051
Training [epoch: 13, batch: 400] loss: 0.051
Training [epoch: 13, batch: 500] loss: 0.053
Training [epoch: 13, batch: 600] loss: 0.052
Training [epoch: 13, batch: 700] loss: 0.054
Training [epoch: 13, batch: 800] loss: 0.055
Training [epoch: 13, batch: 900] loss: 0.056
Training [epoch: 13, batch: 1000] loss: 0.057
Training [epoch: 13, batch: 1100] loss: 0.057
Training [epoch: 13, batch: 1200] loss: 0.056
Training [epoch: 13, batch: 1300] loss: 0.057
Training [epoch: 13, batch: 1400] loss: 0.058
Training [epoch: 13, batch: 1500] loss: 0.059
Training [epoch: 13, batch: 1600] loss: 0.059
Training [epoch: 13, batch: 1700] loss: 0.059
Training [epoch: 13, batch: 1800] loss: 0.059
Training [epoch: 13, batch: 1900] loss: 0.059
Training [epoch: 13, batch: 2000] loss: 0.059
Training [epoch: 13, batch: 2100] loss: 0.059
Training [epoch: 13, batch: 2200] loss: 0.059
Training [epoch: 13, batch: 2300] loss: 0.059
Training [epoch: 13, batch: 2400] loss: 0.060
Training [epoch: 13, batch: 2500] loss: 0.059
Training [epoch: 13, batch: 2600] loss: 0.059
Training [epoch: 13, batch: 2700] loss: 0.059
Training [epoch: 13, batch: 2800] loss: 0.059
Training [epoch: 13, batch: 2900] loss: 0.059
Training [epoch: 13, batch: 3000] loss: 0.059
Training [epoch: 13, batch: 3100] loss: 0.059
Training [epoch: 13, batch: 3200] loss: 0.059
Training [epoch: 13, batch: 3300] loss: 0.059
Training [epoch: 13, batch: 3400] loss: 0.059
Training [epoch: 13, batch: 3500] loss: 0.059
Training [epoch: 13, batch: 3600] loss: 0.059
Training [epoch: 13, batch: 3700] loss: 0.059
Training [epoch: 13, batch: 3800] loss: 0.059
Training [epoch: 13, batch: 3900] loss: 0.059
Training [epoch: 13, batch: 4000] loss: 0.060
Training [epoch: 13, batch: 4100] loss: 0.060
Training [epoch: 13, batch: 4200] loss: 0.060
Training [epoch: 13, batch: 4300] loss: 0.060
Training [epoch: 13, batch: 4400] loss: 0.060
Training [epoch: 13, batch: 4500] loss: 0.060
Training [epoch: 13, batch: 4600] loss: 0.059
Training [epoch: 13, batch: 4700] loss: 0.059
Training [epoch: 13, batch: 4800] loss: 0.059
Training [epoch: 13, batch: 4900] loss: 0.059
Training [epoch: 13, batch: 5000] loss: 0.059
Training [epoch: 13, batch: 5100] loss: 0.060
Training [epoch: 13, batch: 5200] loss: 0.060
Training [epoch: 13, batch: 5300] loss: 0.060
Training [epoch: 13, batch: 5400] loss: 0.060
Training [epoch: 13, batch: 5500] loss: 0.060
Training [epoch: 13, batch: 5600] loss: 0.060
Training [epoch: 13, batch: 5700] loss: 0.059
Training [epoch: 13, batch: 5800] loss: 0.059
Training [epoch: 13, batch: 5900] loss: 0.059
Training [epoch: 13, batch: 6000] loss: 0.059
Training [epoch: 13, batch: 6100] loss: 0.059
Training [epoch: 13, batch: 6200] loss: 0.059
--- 7145.989762067795 seconds for 1 epoch ---
Training [epoch: 14, batch: 100] loss: 0.041
Training [epoch: 14, batch: 200] loss: 0.047
Training [epoch: 14, batch: 300] loss: 0.048
Training [epoch: 14, batch: 400] loss: 0.052
Training [epoch: 14, batch: 500] loss: 0.054
Training [epoch: 14, batch: 600] loss: 0.056
Training [epoch: 14, batch: 700] loss: 0.053
Training [epoch: 14, batch: 800] loss: 0.052
Training [epoch: 14, batch: 900] loss: 0.052
Training [epoch: 14, batch: 1000] loss: 0.051
Training [epoch: 14, batch: 1100] loss: 0.051
Training [epoch: 14, batch: 1200] loss: 0.051
Training [epoch: 14, batch: 1300] loss: 0.051
Training [epoch: 14, batch: 1400] loss: 0.051
Training [epoch: 14, batch: 1500] loss: 0.051
Training [epoch: 14, batch: 1600] loss: 0.052
Training [epoch: 14, batch: 1700] loss: 0.051
Training [epoch: 14, batch: 1800] loss: 0.051
Training [epoch: 14, batch: 1900] loss: 0.051
Training [epoch: 14, batch: 2000] loss: 0.051
Training [epoch: 14, batch: 2100] loss: 0.052
Training [epoch: 14, batch: 2200] loss: 0.051
Training [epoch: 14, batch: 2300] loss: 0.051
Training [epoch: 14, batch: 2400] loss: 0.052
Training [epoch: 14, batch: 2500] loss: 0.051
Training [epoch: 14, batch: 2600] loss: 0.052
Training [epoch: 14, batch: 2700] loss: 0.052
Training [epoch: 14, batch: 2800] loss: 0.052
Training [epoch: 14, batch: 2900] loss: 0.052
Training [epoch: 14, batch: 3000] loss: 0.052
Training [epoch: 14, batch: 3100] loss: 0.053
Training [epoch: 14, batch: 3200] loss: 0.054
Training [epoch: 14, batch: 3300] loss: 0.054
Training [epoch: 14, batch: 3400] loss: 0.054
Training [epoch: 14, batch: 3500] loss: 0.053
Training [epoch: 14, batch: 3600] loss: 0.053
Training [epoch: 14, batch: 3700] loss: 0.053
Training [epoch: 14, batch: 3800] loss: 0.053
Training [epoch: 14, batch: 3900] loss: 0.053
Training [epoch: 14, batch: 4000] loss: 0.052
Training [epoch: 14, batch: 4100] loss: 0.052
Training [epoch: 14, batch: 4200] loss: 0.052
Training [epoch: 14, batch: 4300] loss: 0.052
Training [epoch: 14, batch: 4400] loss: 0.052
Training [epoch: 14, batch: 4500] loss: 0.053
Training [epoch: 14, batch: 4600] loss: 0.053
Training [epoch: 14, batch: 4700] loss: 0.053
Training [epoch: 14, batch: 4800] loss: 0.053
Training [epoch: 14, batch: 4900] loss: 0.053
Training [epoch: 14, batch: 5000] loss: 0.053
Training [epoch: 14, batch: 5100] loss: 0.053
Training [epoch: 14, batch: 5200] loss: 0.053
Training [epoch: 14, batch: 5300] loss: 0.052
Training [epoch: 14, batch: 5400] loss: 0.052
Training [epoch: 14, batch: 5500] loss: 0.052
Training [epoch: 14, batch: 5600] loss: 0.052
Training [epoch: 14, batch: 5700] loss: 0.052
Training [epoch: 14, batch: 5800] loss: 0.052
Training [epoch: 14, batch: 5900] loss: 0.052
Training [epoch: 14, batch: 6000] loss: 0.052
Training [epoch: 14, batch: 6100] loss: 0.052
Training [epoch: 14, batch: 6200] loss: 0.052
--- 7141.157926082611 seconds for 1 epoch ---
Training [epoch: 15, batch: 100] loss: 0.034
Training [epoch: 15, batch: 200] loss: 0.037
Training [epoch: 15, batch: 300] loss: 0.041
Training [epoch: 15, batch: 400] loss: 0.043
Training [epoch: 15, batch: 500] loss: 0.042
Training [epoch: 15, batch: 600] loss: 0.043
Training [epoch: 15, batch: 700] loss: 0.045
Training [epoch: 15, batch: 800] loss: 0.046
Training [epoch: 15, batch: 900] loss: 0.046
Training [epoch: 15, batch: 1000] loss: 0.048
Training [epoch: 15, batch: 1100] loss: 0.047
Training [epoch: 15, batch: 1200] loss: 0.049
Training [epoch: 15, batch: 1300] loss: 0.049
Training [epoch: 15, batch: 1400] loss: 0.050
Training [epoch: 15, batch: 1500] loss: 0.051
Training [epoch: 15, batch: 1600] loss: 0.050
Training [epoch: 15, batch: 1700] loss: 0.050
Training [epoch: 15, batch: 1800] loss: 0.050
Training [epoch: 15, batch: 1900] loss: 0.050
Training [epoch: 15, batch: 2000] loss: 0.050
Training [epoch: 15, batch: 2100] loss: 0.050
Training [epoch: 15, batch: 2200] loss: 0.050
Training [epoch: 15, batch: 2300] loss: 0.050
Training [epoch: 15, batch: 2400] loss: 0.050
Training [epoch: 15, batch: 2500] loss: 0.051
Training [epoch: 15, batch: 2600] loss: 0.051
Training [epoch: 15, batch: 2700] loss: 0.050
Training [epoch: 15, batch: 2800] loss: 0.050
Training [epoch: 15, batch: 2900] loss: 0.050
Training [epoch: 15, batch: 3000] loss: 0.050
Training [epoch: 15, batch: 3100] loss: 0.050
Training [epoch: 15, batch: 3200] loss: 0.050
Training [epoch: 15, batch: 3300] loss: 0.049
Training [epoch: 15, batch: 3400] loss: 0.049
Training [epoch: 15, batch: 3500] loss: 0.050
Training [epoch: 15, batch: 3600] loss: 0.050
Training [epoch: 15, batch: 3700] loss: 0.050
Training [epoch: 15, batch: 3800] loss: 0.050
Training [epoch: 15, batch: 3900] loss: 0.049
Training [epoch: 15, batch: 4000] loss: 0.049
Training [epoch: 15, batch: 4100] loss: 0.049
Training [epoch: 15, batch: 4200] loss: 0.049
Training [epoch: 15, batch: 4300] loss: 0.049
Training [epoch: 15, batch: 4400] loss: 0.048
Training [epoch: 15, batch: 4500] loss: 0.049
Training [epoch: 15, batch: 4600] loss: 0.049
Training [epoch: 15, batch: 4700] loss: 0.048
Training [epoch: 15, batch: 4800] loss: 0.048
Training [epoch: 15, batch: 4900] loss: 0.048
Training [epoch: 15, batch: 5000] loss: 0.048
Training [epoch: 15, batch: 5100] loss: 0.048
Training [epoch: 15, batch: 5200] loss: 0.048
Training [epoch: 15, batch: 5300] loss: 0.048
Training [epoch: 15, batch: 5400] loss: 0.048
Training [epoch: 15, batch: 5500] loss: 0.048
Training [epoch: 15, batch: 5600] loss: 0.048
Training [epoch: 15, batch: 5700] loss: 0.048
Training [epoch: 15, batch: 5800] loss: 0.048
Training [epoch: 15, batch: 5900] loss: 0.048
Training [epoch: 15, batch: 6000] loss: 0.048
Training [epoch: 15, batch: 6100] loss: 0.048
Training [epoch: 15, batch: 6200] loss: 0.048
--- 7132.941666841507 seconds for 1 epoch ---
Training [epoch: 16, batch: 100] loss: 0.038
Training [epoch: 16, batch: 200] loss: 0.042
Training [epoch: 16, batch: 300] loss: 0.039
Training [epoch: 16, batch: 400] loss: 0.043
Training [epoch: 16, batch: 500] loss: 0.044
Training [epoch: 16, batch: 600] loss: 0.042
Training [epoch: 16, batch: 700] loss: 0.041
Training [epoch: 16, batch: 800] loss: 0.041
Training [epoch: 16, batch: 900] loss: 0.042
Training [epoch: 16, batch: 1000] loss: 0.041
Training [epoch: 16, batch: 1100] loss: 0.041
Training [epoch: 16, batch: 1200] loss: 0.042
Training [epoch: 16, batch: 1300] loss: 0.042
Training [epoch: 16, batch: 1400] loss: 0.042
Training [epoch: 16, batch: 1500] loss: 0.042
Training [epoch: 16, batch: 1600] loss: 0.043
Training [epoch: 16, batch: 1700] loss: 0.043
Training [epoch: 16, batch: 1800] loss: 0.043
Training [epoch: 16, batch: 1900] loss: 0.043
Training [epoch: 16, batch: 2000] loss: 0.042
Training [epoch: 16, batch: 2100] loss: 0.042
Training [epoch: 16, batch: 2200] loss: 0.042
Training [epoch: 16, batch: 2300] loss: 0.042
Training [epoch: 16, batch: 2400] loss: 0.042
Training [epoch: 16, batch: 2500] loss: 0.042
Training [epoch: 16, batch: 2600] loss: 0.042
Training [epoch: 16, batch: 2700] loss: 0.042
Training [epoch: 16, batch: 2800] loss: 0.043
Training [epoch: 16, batch: 2900] loss: 0.043
Training [epoch: 16, batch: 3000] loss: 0.043
Training [epoch: 16, batch: 3100] loss: 0.042
Training [epoch: 16, batch: 3200] loss: 0.042
Training [epoch: 16, batch: 3300] loss: 0.042
Training [epoch: 16, batch: 3400] loss: 0.042
Training [epoch: 16, batch: 3500] loss: 0.043
Training [epoch: 16, batch: 3600] loss: 0.042
Training [epoch: 16, batch: 3700] loss: 0.042
Training [epoch: 16, batch: 3800] loss: 0.042
Training [epoch: 16, batch: 3900] loss: 0.042
Training [epoch: 16, batch: 4000] loss: 0.042
Training [epoch: 16, batch: 4100] loss: 0.042
Training [epoch: 16, batch: 4200] loss: 0.042
Training [epoch: 16, batch: 4300] loss: 0.042
Training [epoch: 16, batch: 4400] loss: 0.042
Training [epoch: 16, batch: 4500] loss: 0.042
Training [epoch: 16, batch: 4600] loss: 0.042
Training [epoch: 16, batch: 4700] loss: 0.042
Training [epoch: 16, batch: 4800] loss: 0.042
Training [epoch: 16, batch: 4900] loss: 0.042
Training [epoch: 16, batch: 5000] loss: 0.042
Training [epoch: 16, batch: 5100] loss: 0.042
Training [epoch: 16, batch: 5200] loss: 0.042
Training [epoch: 16, batch: 5300] loss: 0.043
Training [epoch: 16, batch: 5400] loss: 0.043
Training [epoch: 16, batch: 5500] loss: 0.043
Training [epoch: 16, batch: 5600] loss: 0.043
Training [epoch: 16, batch: 5700] loss: 0.043
Training [epoch: 16, batch: 5800] loss: 0.043
Training [epoch: 16, batch: 5900] loss: 0.043
Training [epoch: 16, batch: 6000] loss: 0.043
Training [epoch: 16, batch: 6100] loss: 0.043
Training [epoch: 16, batch: 6200] loss: 0.043
--- 7122.257747888565 seconds for 1 epoch ---
Training [epoch: 17, batch: 100] loss: 0.037
Training [epoch: 17, batch: 200] loss: 0.038
Training [epoch: 17, batch: 300] loss: 0.044
Training [epoch: 17, batch: 400] loss: 0.044
Training [epoch: 17, batch: 500] loss: 0.045
Training [epoch: 17, batch: 600] loss: 0.045
Training [epoch: 17, batch: 700] loss: 0.046
Training [epoch: 17, batch: 800] loss: 0.045
Training [epoch: 17, batch: 900] loss: 0.045
Training [epoch: 17, batch: 1000] loss: 0.044
Training [epoch: 17, batch: 1100] loss: 0.044
Training [epoch: 17, batch: 1200] loss: 0.044
Training [epoch: 17, batch: 1300] loss: 0.044
Training [epoch: 17, batch: 1400] loss: 0.044
Training [epoch: 17, batch: 1500] loss: 0.043
Training [epoch: 17, batch: 1600] loss: 0.043
Training [epoch: 17, batch: 1700] loss: 0.044
Training [epoch: 17, batch: 1800] loss: 0.044
Training [epoch: 17, batch: 1900] loss: 0.044
Training [epoch: 17, batch: 2000] loss: 0.044
Training [epoch: 17, batch: 2100] loss: 0.044
Training [epoch: 17, batch: 2200] loss: 0.044
Training [epoch: 17, batch: 2300] loss: 0.044
Training [epoch: 17, batch: 2400] loss: 0.043
Training [epoch: 17, batch: 2500] loss: 0.043
Training [epoch: 17, batch: 2600] loss: 0.043
Training [epoch: 17, batch: 2700] loss: 0.043
Training [epoch: 17, batch: 2800] loss: 0.043
Training [epoch: 17, batch: 2900] loss: 0.043
Training [epoch: 17, batch: 3000] loss: 0.043
Training [epoch: 17, batch: 3100] loss: 0.042
Training [epoch: 17, batch: 3200] loss: 0.042
Training [epoch: 17, batch: 3300] loss: 0.042
Training [epoch: 17, batch: 3400] loss: 0.042
Training [epoch: 17, batch: 3500] loss: 0.042
Training [epoch: 17, batch: 3600] loss: 0.042
Training [epoch: 17, batch: 3700] loss: 0.041
Training [epoch: 17, batch: 3800] loss: 0.042
Training [epoch: 17, batch: 3900] loss: 0.042
Training [epoch: 17, batch: 4000] loss: 0.042
Training [epoch: 17, batch: 4100] loss: 0.042
Training [epoch: 17, batch: 4200] loss: 0.042
Training [epoch: 17, batch: 4300] loss: 0.042
Training [epoch: 17, batch: 4400] loss: 0.042
Training [epoch: 17, batch: 4500] loss: 0.042
